{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9706d16-f512-4346-b5fd-46c52e9b3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import random\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import FastText\n",
    "from time import time\n",
    "\n",
    "import redis\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pyflink.datastream.functions import RuntimeContext, MapFunction\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream import CheckpointingMode\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7860d1d4-21a6-4d7e-a289-a0c552735aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class for_output(MapFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, value):\n",
    "        return str(value[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cffcb3f-a54b-411d-baba-5dc4d4ba99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class unsupervised_OSA(MapFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        # collection\n",
    "        self.true_label = []\n",
    "        self.collector = []\n",
    "        self.cleaned_text = []\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.collector_size = 2000\n",
    "\n",
    "        # model pruning\n",
    "        self.LRU_index = ['good','bad']\n",
    "        self.max_index = max(self.LRU_index)\n",
    "        self.LRU_cache_size = 30000\n",
    "        self.sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "        # model merging\n",
    "        self.flag = True\n",
    "        self.model_to_train = None\n",
    "        self.timer = time()\n",
    "        self.time_to_reset = 30\n",
    "\n",
    "        # similarity-based classification preparation\n",
    "        self.true_ref_neg = []\n",
    "        self.true_ref_pos = []\n",
    "        self.ref_pos = ['love', 'best', 'beautiful', 'great', 'cool', 'awesome', 'wonderful', 'brilliant', 'excellent',\n",
    "                        'fantastic']\n",
    "        self.ref_neg = ['bad', 'worst', 'stupid', 'disappointing', 'terrible', 'rubbish', 'boring', 'awful',\n",
    "                        'unwatchable', 'awkward']\n",
    "        # self.ref_pos = [self.sno.stem(x) for x in self.ref_pos]\n",
    "        # self.ref_neg = [self.sno.stem(x) for x in self.ref_neg]\n",
    "\n",
    "        # temporal trend detection\n",
    "        self.pos_coefficient = 0.5\n",
    "        self.neg_coefficient = 0.5\n",
    "\n",
    "        # results\n",
    "        self.acc_to_plot = []\n",
    "        self.predictions = []\n",
    "        self.labelled_dataset = ''\n",
    "\n",
    "\n",
    "    def open(self):\n",
    "        # redis-server parameters\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "        # load initial model\n",
    "        self.initial_model = FastText.load('FastText.model')\n",
    "        self.vocabulary = list(self.initial_model.wv.index_to_key)\n",
    "\n",
    "        # save model to redis\n",
    "        self.save_model(self.initial_model)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        try:\n",
    "            self.redis_param.set('fastmodel', pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL))\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Unable to save model to Redis server, please check your model')\n",
    "\n",
    "    def load_model(self):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        try:\n",
    "            called_model = pickle.loads(self.redis_param.get('fastmodel'))\n",
    "            # print(called_model)\n",
    "            return called_model\n",
    "        except TypeError:\n",
    "            logging.info('The model name you entered cannot be found in redis')\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Unable to call the model from Redis server, please check your model')\n",
    "\n",
    "    # tweet preprocessing\n",
    "    def text_to_word_list(self, text):\n",
    "        text = re.sub(\"@\\w+ \", \"\", text)\n",
    "        text = re.sub(\"[!~#$+%*:()'?-]\", ' ', text)\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        clean_word_list = text.strip().split(' ')\n",
    "        clean_word_list = [w for w in clean_word_list if w not in self.stop_words]\n",
    "        while '' in clean_word_list:\n",
    "            clean_word_list.remove('')\n",
    "        self.cleaned_text.append(clean_word_list)\n",
    "        if len(self.cleaned_text) >= self.collector_size:\n",
    "            ans = self.update_model(self.cleaned_text)\n",
    "            return ans\n",
    "        else:\n",
    "            return ('collecting', '1')\n",
    "\n",
    "    def model_prune(self, model):\n",
    "\n",
    "        if len(model.wv.index_to_key) <= self.LRU_cache_size:\n",
    "            return model\n",
    "        else:\n",
    "            word_to_prune = list(self.LRU_index[30000:])\n",
    "            for word in word_to_prune:\n",
    "                k = model.wv.key_to_index[word]\n",
    "                del model.wv.index_to_key[k]\n",
    "                del model.wv.key_to_index[word]\n",
    "            self.vocabulary = list(model.wv.index_to_key)\n",
    "            return model\n",
    "\n",
    "    def get_model_new(self, final_words, final_vectors, final_syn1, final_syn1neg, final_cum_table, corpus_count,\n",
    "                      final_count, final_sample_int, final_code, final_point, model):\n",
    "\n",
    "        model_new = copy.deepcopy(model)\n",
    "        n_words = len(final_words)\n",
    "        model_new.wv.index_to_key = final_words\n",
    "        model_new.wv.key_to_index = {word: idx for idx, word in enumerate(final_words)}\n",
    "        model_new.wv.vectors = final_vectors\n",
    "        model_new.syn1 = final_syn1\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.syn1 = final_syn1\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.cum_table = final_cum_table\n",
    "        model_new.corpus_count = corpus_count\n",
    "        model_new.corpus_total_words = n_words\n",
    "        model_new.wv.expandos['count'] = final_count\n",
    "        model_new.wv.expandos['sample_int'] = final_sample_int\n",
    "        model_new.wv.expandos['code'] = final_code\n",
    "        model_new.wv.expandos['point'] = final_point\n",
    "        return model_new\n",
    "\n",
    "    def model_merge(self, model1, model2):\n",
    "        if model1[0] == 'labelled' and model2[0] == 'labelled':\n",
    "            return (model1[1]) + (model2[1])\n",
    "        elif model1[0] == 'model' and model2[0] == 'model':\n",
    "            model1 = model1[1]\n",
    "            model2 = model2[1]\n",
    "            words1 = copy.deepcopy(model1.wv.index_to_key)\n",
    "            words2 = copy.deepcopy(model2.wv.index_to_key)\n",
    "            syn1s1 = copy.deepcopy(model1.syn1)\n",
    "            syn1s2 = copy.deepcopy(model2.syn1)\n",
    "            syn1negs1 = copy.deepcopy(model1.syn1neg)\n",
    "            syn1negs2 = copy.deepcopy(model2.syn1neg)\n",
    "            cum_tables1 = copy.deepcopy(model1.cum_table)\n",
    "            cum_tables2 = copy.deepcopy(model2.cum_table)\n",
    "            corpus_count = copy.deepcopy(model1.corpus_count) + copy.deepcopy(model2.corpus_count)\n",
    "            counts1 = copy.deepcopy(model1.wv.expandos['count'])\n",
    "            counts2 = copy.deepcopy(model2.wv.expandos['count'])\n",
    "            sample_ints1 = copy.deepcopy(model1.wv.expandos['sample_int'])\n",
    "            sample_ints2 = copy.deepcopy(model2.wv.expandos['sample_int'])\n",
    "            codes1 = copy.deepcopy(model1.wv.expandos['code'])\n",
    "            codes2 = copy.deepcopy(model2.wv.expandos['code'])\n",
    "            points1 = copy.deepcopy(model1.wv.expandos['point'])\n",
    "            points2 = copy.deepcopy(model2.wv.expandos['point'])\n",
    "            final_words = []\n",
    "            final_vectors = []\n",
    "            final_syn1 = []\n",
    "            final_syn1neg = []\n",
    "            final_cum_table = []\n",
    "            final_count = []\n",
    "            final_sample_int = []\n",
    "            final_code = []\n",
    "            final_point = []\n",
    "            for idx1 in range(len(words1)):\n",
    "                word = words1[idx1]\n",
    "                v1 = model1.wv[word]\n",
    "                syn11 = syn1s1[idx1]\n",
    "                syn1neg1 = syn1negs1[idx1]\n",
    "                cum_table1 = cum_tables1[idx1]\n",
    "                count = counts1[idx1]\n",
    "                sample_int = sample_ints1[idx1]\n",
    "                code = codes1[idx1]\n",
    "                point = points1[idx1]\n",
    "                try:\n",
    "                    idx2 = words2.index(word)\n",
    "                    v2 = model2.wv[word]\n",
    "                    syn12 = syn1s2[idx2]\n",
    "                    syn1neg2 = syn1negs2[idx2]\n",
    "                    cum_table2 = cum_tables2[idx2]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v1\n",
    "                    syn1 = syn11\n",
    "                    syn1neg = syn1neg1\n",
    "                    cum_table = cum_table1\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "            for idx2 in range(len(words2)):\n",
    "                word = words2[idx2]\n",
    "                if word in final_words:\n",
    "                    continue\n",
    "                v2 = model2.wv[word]\n",
    "                syn12 = syn1s2[idx2]\n",
    "                syn1neg2 = syn1negs2[idx2]\n",
    "                cum_table2 = cum_tables2[idx2]\n",
    "                count = counts2[idx2]\n",
    "                sample_int = sample_ints2[idx2]\n",
    "                code = codes2[idx2]\n",
    "                point = points2[idx2]\n",
    "                try:\n",
    "                    idx1 = words1.index(word)\n",
    "                    v1 = model1.wv[word]\n",
    "                    syn11 = syn1s1[idx1]\n",
    "                    syn1neg1 = syn1negs1[idx1]\n",
    "                    cum_table1 = cum_tables1[idx1]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v2\n",
    "                    syn1 = syn12\n",
    "                    syn1neg = syn1neg2\n",
    "                    cum_table = cum_table2\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "\n",
    "            model_new = self.get_model_new(final_words, np.array(final_vectors), np.array(final_syn1),\n",
    "                                           np.array(final_syn1neg), \\\n",
    "                                           final_cum_table, corpus_count, np.array(final_count),\n",
    "                                           np.array(final_sample_int), \\\n",
    "                                           np.array(final_code), np.array(final_point), model1)\n",
    "            self.save_model(model_new)\n",
    "            self.flag = True\n",
    "            return model_new\n",
    "        elif model1[0] == 'model':\n",
    "            self.save_model(model1[0])\n",
    "            self.flag = True\n",
    "            return model1[0]\n",
    "        else:\n",
    "            self.save_model(model2[0])\n",
    "            self.flag = True\n",
    "            return model2[0]\n",
    "            \n",
    "    def map(self, tweet):\n",
    "        # with open('hello.txt','a') as wr:\n",
    "        #     wr.write(tweet[0],twe)\n",
    "        self.true_label.append(int(tweet[1]))\n",
    "        self.collector.append(tweet[0])\n",
    "        return self.text_to_word_list(tweet[0])\n",
    "\n",
    "    def update_model(self, new_sentences):\n",
    "\n",
    "        if self.flag:\n",
    "            call_model = self.load_model()\n",
    "            self.flag = False\n",
    "        else:\n",
    "            call_model = self.model_to_train\n",
    "\n",
    "        # incremental learning\n",
    "        call_model.build_vocab(new_sentences, update=True)  # 1) update vocabulary\n",
    "        call_model.train(new_sentences,  # 2) incremental training\n",
    "                         total_examples=call_model.corpus_count,\n",
    "                         epochs=call_model.epochs,\n",
    "                        )\n",
    "        for word in call_model.wv.index_to_key:\n",
    "            if word not in self.vocabulary:  # new words\n",
    "                self.LRU_index.insert(0, word)\n",
    "            else:  # duplicate words\n",
    "                self.LRU_index.remove(word)\n",
    "                self.LRU_index.insert(0, word)\n",
    "        self.vocabulary = list(call_model.wv.index_to_key)\n",
    "        self.model_to_train = call_model\n",
    "\n",
    "        if len(self.ref_neg) > 0:\n",
    "            for words in self.ref_neg:\n",
    "                if words in call_model.wv:\n",
    "                    self.ref_neg.remove(words)\n",
    "                    if words not in self.true_ref_neg:\n",
    "                        self.true_ref_neg.append(words)\n",
    "        if len(self.ref_pos) > 0:\n",
    "            for words in self.ref_pos:\n",
    "                if words in call_model.wv:\n",
    "                    self.ref_pos.remove(words)\n",
    "                    if words not in self.true_ref_pos:\n",
    "                        self.true_ref_pos.append(words)\n",
    "\n",
    "        classify_result = self.eval(new_sentences, call_model)\n",
    "        self.cleaned_text = []\n",
    "        self.true_label = []\n",
    "\n",
    "        if time() - self.timer >= self.time_to_reset:\n",
    "            call_model = self.model_prune(call_model)\n",
    "            model_to_merge = ('model', call_model)\n",
    "            self.timer = time()\n",
    "            return model_to_merge\n",
    "        else:\n",
    "            not_yet = ('labelled', classify_result)\n",
    "            return not_yet\n",
    "\n",
    "    def eval(self, tweets, model):\n",
    "        for i in range(len(tweets)):\n",
    "            predict_result = self.predict(tweets[i], model)\n",
    "            self.predictions.append(predict_result)\n",
    "            self.labelled_dataset += (self.collector[i]+' '+predict_result+'@@@@')\n",
    "        self.neg_coefficient = self.predictions.count('0')/(self.predictions.count('0')+self.predictions.count('1'))\n",
    "        self.pos_coefficient = 1 - self.neg_coefficient\n",
    "        self.predictions = []\n",
    "        self.collector = []\n",
    "        ans = self.labelled_dataset\n",
    "        return ans\n",
    "\n",
    "    def predict(self, tweet, model):\n",
    "        sentence = np.zeros(20)\n",
    "        counter = 0\n",
    "        cos_sim_bad, cos_sim_good = 0, 0\n",
    "        for words in tweet:\n",
    "\n",
    "            try:\n",
    "                sentence += model.wv[words]  # np.array(list(model.wv[words]) + new_feature)\n",
    "                counter += 1\n",
    "            except:\n",
    "                pass\n",
    "        if counter != 0:\n",
    "            sentence_vec = sentence / counter\n",
    "        k_cur = min(len(self.true_ref_neg), len(self.true_ref_pos))\n",
    "        for neg_word in self.true_ref_neg[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_bad += dot(sentence_vec, model.wv[neg_word]) / (norm(sentence_vec) * norm(model.wv[neg_word]))\n",
    "            except:\n",
    "                pass\n",
    "        for pos_word in self.true_ref_pos[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_good += dot(sentence_vec, model.wv[pos_word]) / (norm(sentence_vec) * norm(model.wv[pos_word]))\n",
    "            except:\n",
    "                pass\n",
    "        if cos_sim_bad - cos_sim_good > 0.5:\n",
    "            return '0'\n",
    "        elif cos_sim_bad - cos_sim_good < -0.5:\n",
    "            return '1'\n",
    "        else:\n",
    "            if cos_sim_bad * self.neg_coefficient >= cos_sim_good * self.pos_coefficient:\n",
    "                return '0'\n",
    "            else:\n",
    "                return '1'\n",
    "\n",
    "    def write_log(self,file,msg):\n",
    "        with open(file, 'a') as wr:\n",
    "            wr.write(msg + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2190da66-1e63-4199-8941-cd4a58cda39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UO:\n",
    "    def __init__(self,obj):\n",
    "        self.object=obj\n",
    "        self.stream=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f048dab0-c413-4a97-871b-a59940d249f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelism = 4\n",
    "# the labels of dataset are only used for accuracy computation, since PLStream is unsupervised\n",
    "f = pd.read_csv('./train.csv')  # , encoding='ISO-8859-1'\n",
    "f.columns = [\"label\",\"review\"]\n",
    "\n",
    "# 20,000 data for quick testing\n",
    "n =20000\n",
    "true_label = list(f.label)[:n]\n",
    "for i in range(len(true_label)):\n",
    "    if true_label[i] == 1:\n",
    "        true_label[i] = 0\n",
    "    else:\n",
    "        true_label[i] = 1\n",
    "yelp_review = list(f.review)[:n]\n",
    "data_stream = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8880db1-769a-4f9f-b446-785af948d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets=[]\n",
    "parallelism = 4\n",
    "# u= unsupervised_OSA\n",
    "for i in range(parallelism):\n",
    "    buckets.append(UO(unsupervised_OSA()))\n",
    "    buckets[i].object.open()\n",
    "    \n",
    "for j in range(len(yelp_review)):\n",
    "        bucket=buckets[j%parallelism]\n",
    "        bucket.stream.append(bucket.object.map([yelp_review[j],int(true_label[j])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cc91649-ef17-4be9-b034-6c70301eacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(parallelism):\n",
    "    stream=buckets[i].stream\n",
    "    stream=pd.DataFrame(stream)\n",
    "    stream=stream[stream[0]!='collecting']\n",
    "    buckets[i].stream=stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ac2b46-3b9f-4a4c-8a28-7a97d9a77525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(buckets[0].stream.shape[0]):\n",
    "    for j in range(0,len(buckets),2):\n",
    "        nm=buckets[j].object.model_merge(buckets[j].stream.iloc[i],buckets[j+1].stream.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2254398-bfeb-4ac6-b007-5ef35a9d6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "att=buckets[0].object.model_to_train.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e716800-bfb3-43e1-b045-855e9177cee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['count', 'sample_int', 'code', 'point'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.wv.expandos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c7e418-a426-4265-ae7c-72787080ea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['load',\n",
       " 'load_fasttext_format',\n",
       " 'callbacks',\n",
       " 'word_ngrams',\n",
       " 'wv',\n",
       " 'vector_size',\n",
       " 'workers',\n",
       " 'epochs',\n",
       " 'train_count',\n",
       " 'total_train_time',\n",
       " 'batch_words',\n",
       " 'sg',\n",
       " 'alpha',\n",
       " 'min_alpha',\n",
       " 'window',\n",
       " 'shrink_windows',\n",
       " 'random',\n",
       " 'hs',\n",
       " 'negative',\n",
       " 'ns_exponent',\n",
       " 'cbow_mean',\n",
       " 'compute_loss',\n",
       " 'running_training_loss',\n",
       " 'min_alpha_yet_reached',\n",
       " 'corpus_count',\n",
       " 'corpus_total_words',\n",
       " 'max_final_vocab',\n",
       " 'max_vocab_size',\n",
       " 'min_count',\n",
       " 'sample',\n",
       " 'sorted_vocab',\n",
       " 'null_word',\n",
       " 'raw_vocab',\n",
       " 'hashfxn',\n",
       " 'seed',\n",
       " 'layer1_size',\n",
       " 'comment',\n",
       " 'effective_min_count',\n",
       " 'lifecycle_events',\n",
       " 'syn1',\n",
       " 'syn1neg',\n",
       " '__numpys',\n",
       " '__scipys',\n",
       " '__ignoreds',\n",
       " '__recursive_saveloads',\n",
       " 'cum_table',\n",
       " '__module__',\n",
       " '__init__',\n",
       " '_init_post_load',\n",
       " '_clear_post_train',\n",
       " 'estimate_memory',\n",
       " '_do_train_epoch',\n",
       " '_do_train_job',\n",
       " 'init_sims',\n",
       " 'load_binary_data',\n",
       " 'save',\n",
       " '_load_specials',\n",
       " '__doc__',\n",
       " '__slotnames__',\n",
       " 'build_vocab',\n",
       " 'build_vocab_from_freq',\n",
       " '_scan_vocab',\n",
       " 'scan_vocab',\n",
       " 'prepare_vocab',\n",
       " 'add_null_word',\n",
       " 'create_binary_tree',\n",
       " 'make_cum_table',\n",
       " 'prepare_weights',\n",
       " 'seeded_vector',\n",
       " 'init_weights',\n",
       " 'update_weights',\n",
       " 'train',\n",
       " '_worker_loop_corpusfile',\n",
       " '_worker_loop',\n",
       " '_job_producer',\n",
       " '_log_epoch_progress',\n",
       " '_train_epoch_corpusfile',\n",
       " '_train_epoch',\n",
       " '_get_next_alpha',\n",
       " '_get_thread_working_mem',\n",
       " '_raw_word_count',\n",
       " '_check_corpus_sanity',\n",
       " '_check_training_sanity',\n",
       " '_log_progress',\n",
       " '_log_epoch_end',\n",
       " '_log_train_end',\n",
       " 'score',\n",
       " 'predict_output_word',\n",
       " 'reset_from',\n",
       " '__str__',\n",
       " '_save_specials',\n",
       " 'get_latest_training_loss',\n",
       " 'add_lifecycle_event',\n",
       " '_adapt_by_suffix',\n",
       " '_smart_save',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898f2af-7aba-4249-abe6-390ae40630e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = FastText.load('FastText.model')\n",
    "# self.vocabulary = list(self.initial_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85500d0f-6dc3-4ecd-abdf-32d67c6e9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "876bc024-0613-4301-a882-963960dc018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>labelled</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>labelled</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                                                  1\n",
       "1999  labelled  All the food is great here. But the best thing...\n",
       "3999  labelled  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets[3].stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761338a-5ca5-457b-9810-24cc6ef65aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
