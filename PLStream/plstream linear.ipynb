{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870ef5f4-671b-4c4e-9035-e2c81e33c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import redis\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pyflink.datastream.functions import RuntimeContext, MapFunction\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream import CheckpointingMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596625c1-e630-436a-a717-d6780a06fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class for_output(MapFunction):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, value):\n",
    "        return str(value[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c263f315-fe12-4b55-8926-4de3786a17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class unsupervised_OSA(MapFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        # collection\n",
    "        self.true_label = []\n",
    "        self.collector = []\n",
    "        self.cleaned_text = []\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.collector_size = 2000\n",
    "\n",
    "        # model pruning\n",
    "        self.LRU_index = ['good','bad']\n",
    "        self.max_index = max(self.LRU_index)\n",
    "        self.LRU_cache_size = 30000\n",
    "        self.sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "        # model merging\n",
    "        self.flag = True\n",
    "        self.model_to_train = None\n",
    "        self.timer = time()\n",
    "        self.time_to_reset = 30\n",
    "\n",
    "        # similarity-based classification preparation\n",
    "        self.true_ref_neg = []\n",
    "        self.true_ref_pos = []\n",
    "        self.ref_pos = ['love', 'best', 'beautiful', 'great', 'cool', 'awesome', 'wonderful', 'brilliant', 'excellent',\n",
    "                        'fantastic']\n",
    "        self.ref_neg = ['bad', 'worst', 'stupid', 'disappointing', 'terrible', 'rubbish', 'boring', 'awful',\n",
    "                        'unwatchable', 'awkward']\n",
    "        # self.ref_pos = [self.sno.stem(x) for x in self.ref_pos]\n",
    "        # self.ref_neg = [self.sno.stem(x) for x in self.ref_neg]\n",
    "\n",
    "        # temporal trend detection\n",
    "        self.pos_coefficient = 0.5\n",
    "        self.neg_coefficient = 0.5\n",
    "\n",
    "        # results\n",
    "        self.acc_to_plot = []\n",
    "        self.predictions = []\n",
    "        self.labelled_dataset = ''\n",
    "\n",
    "\n",
    "    def open(self):\n",
    "        # redis-server parameters\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "        # load initial model\n",
    "        self.initial_model = Word2Vec.load('PLS_c10.model')\n",
    "        self.vocabulary = list(self.initial_model.wv.index_to_key)\n",
    "\n",
    "        # save model to redis\n",
    "        self.save_model(self.initial_model)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        try:\n",
    "            self.redis_param.set('osamodel', pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL))\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Unable to save model to Redis server, please check your model')\n",
    "\n",
    "    def load_model(self):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        try:\n",
    "            called_model = pickle.loads(self.redis_param.get('osamodel'))\n",
    "            return called_model\n",
    "        except TypeError:\n",
    "            logging.info('The model name you entered cannot be found in redis')\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Unable to call the model from Redis server, please check your model')\n",
    "\n",
    "    # tweet preprocessing\n",
    "    def text_to_word_list(self, text):\n",
    "        text = re.sub(\"@\\w+ \", \"\", text)\n",
    "        text = re.sub(\"[!~#$+%*:()'?-]\", ' ', text)\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        clean_word_list = text.strip().split(' ')\n",
    "        clean_word_list = [w for w in clean_word_list if w not in self.stop_words]\n",
    "        while '' in clean_word_list:\n",
    "            clean_word_list.remove('')\n",
    "        self.cleaned_text.append(clean_word_list)\n",
    "        if len(self.cleaned_text) >= self.collector_size:\n",
    "            ans = self.update_model(self.cleaned_text)\n",
    "            return ans\n",
    "        else:\n",
    "            return ('collecting', '1')\n",
    "\n",
    "    def model_prune(self, model):\n",
    "\n",
    "        if len(model.wv.index_to_key) <= self.LRU_cache_size:\n",
    "            return model\n",
    "        else:\n",
    "            word_to_prune = list(self.LRU_index[30000:])\n",
    "            for word in word_to_prune:\n",
    "                k = model.wv.key_to_index[word]\n",
    "                del model.wv.index_to_key[k]\n",
    "                del model.wv.key_to_index[word]\n",
    "            self.vocabulary = list(model.wv.index_to_key)\n",
    "            return model\n",
    "\n",
    "    def get_model_new(self, final_words, final_vectors, final_syn1, final_syn1neg, final_cum_table, corpus_count,\n",
    "                      final_count, final_sample_int, final_code, final_point, model):\n",
    "\n",
    "        model_new = copy.deepcopy(model)\n",
    "        n_words = len(final_words)\n",
    "        model_new.wv.index_to_key = final_words\n",
    "        model_new.wv.key_to_index = {word: idx for idx, word in enumerate(final_words)}\n",
    "        model_new.wv.vectors = final_vectors\n",
    "        model_new.syn1 = final_syn1\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.syn1 = final_syn1\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.cum_table = final_cum_table\n",
    "        model_new.corpus_count = corpus_count\n",
    "        model_new.corpus_total_words = n_words\n",
    "        model_new.wv.expandos['count'] = final_count\n",
    "        model_new.wv.expandos['sample_int'] = final_sample_int\n",
    "        model_new.wv.expandos['code'] = final_code\n",
    "        model_new.wv.expandos['point'] = final_point\n",
    "        return model_new\n",
    "\n",
    "    def model_merge(self, model1, model2):\n",
    "        if model1[0] == 'labelled':\n",
    "            return (model1[1]) + (model2[1])\n",
    "        elif model1[0] == 'model':\n",
    "            model1 = model1[1]\n",
    "            model2 = model2[1]\n",
    "            words1 = copy.deepcopy(model1.wv.index_to_key)\n",
    "            words2 = copy.deepcopy(model2.wv.index_to_key)\n",
    "            syn1s1 = copy.deepcopy(model1.syn1)\n",
    "            syn1s2 = copy.deepcopy(model2.syn1)\n",
    "            syn1negs1 = copy.deepcopy(model1.syn1neg)\n",
    "            syn1negs2 = copy.deepcopy(model2.syn1neg)\n",
    "            cum_tables1 = copy.deepcopy(model1.cum_table)\n",
    "            cum_tables2 = copy.deepcopy(model2.cum_table)\n",
    "            corpus_count = copy.deepcopy(model1.corpus_count) + copy.deepcopy(model2.corpus_count)\n",
    "            counts1 = copy.deepcopy(model1.wv.expandos['count'])\n",
    "            counts2 = copy.deepcopy(model2.wv.expandos['count'])\n",
    "            sample_ints1 = copy.deepcopy(model1.wv.expandos['sample_int'])\n",
    "            sample_ints2 = copy.deepcopy(model2.wv.expandos['sample_int'])\n",
    "            codes1 = copy.deepcopy(model1.wv.expandos['code'])\n",
    "            codes2 = copy.deepcopy(model2.wv.expandos['code'])\n",
    "            points1 = copy.deepcopy(model1.wv.expandos['point'])\n",
    "            points2 = copy.deepcopy(model2.wv.expandos['point'])\n",
    "            final_words = []\n",
    "            final_vectors = []\n",
    "            final_syn1 = []\n",
    "            final_syn1neg = []\n",
    "            final_cum_table = []\n",
    "            final_count = []\n",
    "            final_sample_int = []\n",
    "            final_code = []\n",
    "            final_point = []\n",
    "            for idx1 in range(len(words1)):\n",
    "                word = words1[idx1]\n",
    "                v1 = model1.wv[word]\n",
    "                syn11 = syn1s1[idx1]\n",
    "                syn1neg1 = syn1negs1[idx1]\n",
    "                cum_table1 = cum_tables1[idx1]\n",
    "                count = counts1[idx1]\n",
    "                sample_int = sample_ints1[idx1]\n",
    "                code = codes1[idx1]\n",
    "                point = points1[idx1]\n",
    "                try:\n",
    "                    idx2 = words2.index(word)\n",
    "                    v2 = model2.wv[word]\n",
    "                    syn12 = syn1s2[idx2]\n",
    "                    syn1neg2 = syn1negs2[idx2]\n",
    "                    cum_table2 = cum_tables2[idx2]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v1\n",
    "                    syn1 = syn11\n",
    "                    syn1neg = syn1neg1\n",
    "                    cum_table = cum_table1\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "            for idx2 in range(len(words2)):\n",
    "                word = words2[idx2]\n",
    "                if word in final_words:\n",
    "                    continue\n",
    "                v2 = model2.wv[word]\n",
    "                syn12 = syn1s2[idx2]\n",
    "                syn1neg2 = syn1negs2[idx2]\n",
    "                cum_table2 = cum_tables2[idx2]\n",
    "                count = counts2[idx2]\n",
    "                sample_int = sample_ints2[idx2]\n",
    "                code = codes2[idx2]\n",
    "                point = points2[idx2]\n",
    "                try:\n",
    "                    idx1 = words1.index(word)\n",
    "                    v1 = model1.wv[word]\n",
    "                    syn11 = syn1s1[idx1]\n",
    "                    syn1neg1 = syn1negs1[idx1]\n",
    "                    cum_table1 = cum_tables1[idx1]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v2\n",
    "                    syn1 = syn12\n",
    "                    syn1neg = syn1neg2\n",
    "                    cum_table = cum_table2\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "\n",
    "            model_new = self.get_model_new(final_words, np.array(final_vectors), np.array(final_syn1),\n",
    "                                           np.array(final_syn1neg), \\\n",
    "                                           final_cum_table, corpus_count, np.array(final_count),\n",
    "                                           np.array(final_sample_int), \\\n",
    "                                           np.array(final_code), np.array(final_point), model1)\n",
    "            self.save_model(model_new)\n",
    "            self.flag = True\n",
    "            return model_new\n",
    "\n",
    "    def map(self, tweet):\n",
    "        # with open('hello.txt','a') as wr:\n",
    "        #     wr.write(tweet[0],twe)\n",
    "        self.true_label.append(int(tweet[1]))\n",
    "        self.collector.append(tweet[0])\n",
    "        return self.text_to_word_list(tweet[0])\n",
    "\n",
    "    def update_model(self, new_sentences):\n",
    "\n",
    "        if self.flag:\n",
    "            call_model = self.load_model()\n",
    "            self.flag = False\n",
    "        else:\n",
    "            call_model = self.model_to_train\n",
    "\n",
    "        # incremental learning\n",
    "        call_model.build_vocab(new_sentences, update=True)  # 1) update vocabulary\n",
    "        call_model.train(new_sentences,  # 2) incremental training\n",
    "                         total_examples=call_model.corpus_count,\n",
    "                         epochs=call_model.epochs)\n",
    "        for word in call_model.wv.index_to_key:\n",
    "            if word not in self.vocabulary:  # new words\n",
    "                self.LRU_index.insert(0, word)\n",
    "            else:  # duplicate words\n",
    "                self.LRU_index.remove(word)\n",
    "                self.LRU_index.insert(0, word)\n",
    "        self.vocabulary = list(call_model.wv.index_to_key)\n",
    "        self.model_to_train = call_model\n",
    "\n",
    "        if len(self.ref_neg) > 0:\n",
    "            for words in self.ref_neg:\n",
    "                if words in call_model.wv:\n",
    "                    self.ref_neg.remove(words)\n",
    "                    if words not in self.true_ref_neg:\n",
    "                        self.true_ref_neg.append(words)\n",
    "        if len(self.ref_pos) > 0:\n",
    "            for words in self.ref_pos:\n",
    "                if words in call_model.wv:\n",
    "                    self.ref_pos.remove(words)\n",
    "                    if words not in self.true_ref_pos:\n",
    "                        self.true_ref_pos.append(words)\n",
    "\n",
    "        classify_result = self.eval(new_sentences, call_model)\n",
    "        self.cleaned_text = []\n",
    "        self.true_label = []\n",
    "\n",
    "        if time() - self.timer >= self.time_to_reset:\n",
    "            call_model = self.model_prune(call_model)\n",
    "            model_to_merge = ('model', call_model)\n",
    "            self.timer = time()\n",
    "            return model_to_merge\n",
    "        else:\n",
    "            not_yet = ('labelled', classify_result)\n",
    "            return not_yet\n",
    "\n",
    "    def eval(self, tweets, model):\n",
    "        for i in range(len(tweets)):\n",
    "            predict_result = self.predict(tweets[i], model)\n",
    "            self.predictions.append(predict_result)\n",
    "            self.labelled_dataset += (self.collector[i]+' '+predict_result+'@@@@')\n",
    "        self.neg_coefficient = self.predictions.count('0')/(self.predictions.count('0')+self.predictions.count('1'))\n",
    "        self.pos_coefficient = 1 - self.neg_coefficient\n",
    "        self.predictions = []\n",
    "        self.collector = []\n",
    "        ans = self.labelled_dataset\n",
    "        return ans\n",
    "\n",
    "    def predict(self, tweet, model):\n",
    "        sentence = np.zeros(20)\n",
    "        counter = 0\n",
    "        cos_sim_bad, cos_sim_good = 0, 0\n",
    "        for words in tweet:\n",
    "\n",
    "            try:\n",
    "                sentence += model.wv[words]  # np.array(list(model.wv[words]) + new_feature)\n",
    "                counter += 1\n",
    "            except:\n",
    "                pass\n",
    "        if counter != 0:\n",
    "            sentence_vec = sentence / counter\n",
    "        k_cur = min(len(self.true_ref_neg), len(self.true_ref_pos))\n",
    "        for neg_word in self.true_ref_neg[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_bad += dot(sentence_vec, model.wv[neg_word]) / (norm(sentence_vec) * norm(model.wv[neg_word]))\n",
    "            except:\n",
    "                pass\n",
    "        for pos_word in self.true_ref_pos[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_good += dot(sentence_vec, model.wv[pos_word]) / (norm(sentence_vec) * norm(model.wv[pos_word]))\n",
    "            except:\n",
    "                pass\n",
    "        if cos_sim_bad - cos_sim_good > 0.5:\n",
    "            return '0'\n",
    "        elif cos_sim_bad - cos_sim_good < -0.5:\n",
    "            return '1'\n",
    "        else:\n",
    "            if cos_sim_bad * self.neg_coefficient >= cos_sim_good * self.pos_coefficient:\n",
    "                return '0'\n",
    "            else:\n",
    "                return '1'\n",
    "\n",
    "    def write_log(self,file,msg):\n",
    "        with open(file, 'a') as wr:\n",
    "            wr.write(msg + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287fa54d-d73d-4c97-aa4f-4a1165ec162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from pyflink.datastream.connectors import StreamingFileSink\n",
    "    from pyflink.common.serialization import Encoder\n",
    "    from time import time\n",
    "    import pandas as pd\n",
    "\n",
    "    parallelism = 4\n",
    "    # the labels of dataset are only used for accuracy computation, since PLStream is unsupervised\n",
    "    f = pd.read_csv('./train.csv')  # , encoding='ISO-8859-1'\n",
    "    f.columns = [\"label\",\"review\"]\n",
    "    \n",
    "    # 20,000 data for quick testing\n",
    "    n =20000\n",
    "    true_label = list(f.label)[:n]\n",
    "    for i in range(len(true_label)):\n",
    "        if true_label[i] == 1:\n",
    "            true_label[i] = 0\n",
    "        else:\n",
    "            true_label[i] = 1\n",
    "    yelp_review = list(f.review)[:n]\n",
    "    data_stream = []\n",
    "    # for i in range(len(yelp_review)):\n",
    "    #     data_stream.append((yelp_review[i], int(true_label[i])))\n",
    "    # print('Coming Stream is ready...')\n",
    "    # print('===============================')\n",
    "\n",
    "    # env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # env.set_parallelism(1)\n",
    "    # env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)\n",
    "    # ds = env.from_collection(collection=data_stream)\n",
    "    # ds.map(unsupervised_OSA()).set_parallelism(parallelism) \\\n",
    "    #     .filter(lambda x: x[0] != 'collecting') \\\n",
    "    #     .key_by(lambda x: x[0], key_type=Types.STRING()) \\\n",
    "    #     .reduce(lambda x, y: (x[0], unsupervised_OSA().model_merge(x, y))).set_parallelism(2) \\\n",
    "    #     .filter(lambda x: x[0] != 'model') \\\n",
    "    #     .map(for_output(), output_type=Types.STRING()).set_parallelism(1) \\\n",
    "    #     .add_sink(StreamingFileSink  # .set_parallelism(2)\n",
    "    #               .for_row_format('./output', Encoder.simple_string_encoder())\n",
    "    #               .build())\n",
    "    # env.execute(\"osa_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3964f-d627-4d0d-bf3a-8a33ccc48907",
   "metadata": {},
   "source": [
    "model has to return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51c93c8-03ba-442f-86cc-e340bcf7221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UO:\n",
    "    def __init__(self,obj):\n",
    "        self.object=obj\n",
    "        self.stream=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf177f0-f7f5-4b95-bdd4-7f715c377f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelism = 4\n",
    "# the labels of dataset are only used for accuracy computation, since PLStream is unsupervised\n",
    "f = pd.read_csv('./train.csv')  # , encoding='ISO-8859-1'\n",
    "f.columns = [\"label\",\"review\"]\n",
    "\n",
    "# 20,000 data for quick testing\n",
    "n =20000\n",
    "true_label = list(f.label)[:n]\n",
    "for i in range(len(true_label)):\n",
    "    if true_label[i] == 1:\n",
    "        true_label[i] = 0\n",
    "    else:\n",
    "        true_label[i] = 1\n",
    "yelp_review = list(f.review)[:n]\n",
    "data_stream = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91786159-5df1-4d40-84c4-763b2e180c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets=[]\n",
    "parallelism = 4\n",
    "# u= unsupervised_OSA\n",
    "for i in range(parallelism):\n",
    "    buckets.append(UO(unsupervised_OSA()))\n",
    "    buckets[i].object.open()\n",
    "    \n",
    "for j in range(len(yelp_review)):\n",
    "        bucket=buckets[j%parallelism]\n",
    "        bucket.stream.append(bucket.object.map([yelp_review[j],int(true_label[j])]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457dd339-bc53-44ad-9d1b-a5a7ade412e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(parallelism):\n",
    "    stream=buckets[i].stream\n",
    "    stream=pd.DataFrame(stream)\n",
    "    stream=stream[stream[0]!='collecting']\n",
    "    buckets[i].stream=stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548a643d-3ef3-4384-9724-82b088cd8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(buckets[0].stream.shape[0]):\n",
    "    for j in range(0,len(buckets),2):\n",
    "        nm=buckets[j].object.model_merge(buckets[j].stream.iloc[i],buckets[j+1].stream.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9aa614-33c0-43c8-9427-a432f1e0ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ats=buckets[0].object.model_to_train.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8da93d2-d50b-4e41-8b88-fa0b7b0d583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=buckets[0].object.model_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7f2f76-a855-40bc-a7ea-a65497cad0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0046c29-132a-440d-a6dd-00f5e437a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['count', 'sample_int', 'code', 'point'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.wv.expandos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040e6e0-945d-4779-bb60-bf2dfcb6c5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed504cf4-bd06-4fd6-9d41-d6ec756f7143",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_101598/3893355526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/PLStream-d4oWSIgs/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         raise AttributeError(\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;34m\".get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "m.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a49e14c6-52f7-4ffd-a6f8-7f93133839e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee0fc503-ee26-4230-a0ab-a11efcc01429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cab2457-bedb-4657-b1e7-99a3bd341f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_128097/1500518560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m.wv.expandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cdd50cc-4906-4878-b238-e591c2949bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=ats[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "86a31a41-3757-40b7-b2c2-6b9881367825",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=interested[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7651f867-8b75-4473-a57a-43bb5028e5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpus_count', 'corpus_total_words', 'wv', 'syn1', 'syn1neg', 'cum_table']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88eac7-70f5-4132-ae47-aa053d541a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9cf86-e621-4f5f-82dd-e0041420ea8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
