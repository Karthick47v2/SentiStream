{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS FOR CONVERTING TWEETER DATASET INTO STREAM DATASET (arranged by time)\n",
    "\n",
    "def refactor_date(txt):\n",
    "    txt = txt.split(' ')[1:3]\n",
    "\n",
    "    code = 0\n",
    "\n",
    "    if txt[0] == 'May':\n",
    "        code = 1\n",
    "    elif txt[0] == 'Jun':\n",
    "        code = 2\n",
    "\n",
    "    return code * 31 + int(txt[1])\n",
    "\n",
    "# preprocessing data to be like datastream\n",
    "df = pd.read_csv('tweet_unprocessed.csv',encoding=\"ISO-8859-1\",names=['label','ids','date','flag','user','review'])\n",
    "df['date'] = df['date'].apply(refactor_date)\n",
    "df.sort_values(by=['date'], inplace=True)\n",
    "df.drop(['ids', 'flag', 'user'], axis=1, inplace=True)\n",
    "df = df.append({'label': 0, 'date': 88, 'review': 'Placeholder'}, ignore_index=True)\n",
    "df.to_csv('tweet_processed.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER DATASET TRAIN PARAMS\n",
    "\n",
    "# DAY 1\n",
    "\n",
    "# W2V\n",
    "#### W2V_VEC_DIM = 10\n",
    "#### W2V_WINDOW = 5\n",
    "#### W2V_MIN_COUNT = 3\n",
    "#### W2V_EPOCHS = 10\n",
    "\n",
    "# NN CLF\n",
    "#### CLF_BATCH_SIZE = 64\n",
    "#### CLF_HIDDEN_LAYER1_DIM = 64\n",
    "#### CLF_LEARNING_RATE = 1e-4\n",
    "#### CLF_EPOCHS = 500\n",
    "\n",
    "\n",
    "# DAY 1 & 2\n",
    "# W2V\n",
    "#### W2V_VEC_DIM = 50\n",
    "#### W2V_WINDOW = 5\n",
    "#### W2V_MIN_COUNT = 3\n",
    "#### W2V_EPOCHS = 10\n",
    "\n",
    "# NN CLF\n",
    "#### CLF_BATCH_SIZE = 128\n",
    "#### CLF_HIDDEN_LAYER1_DIM = 32\n",
    "#### CLF_HIDDEN_LAYER2_DIM = 16\n",
    "#### CLF_LEARNING_RATE = 5e-4\n",
    "#### CLF_EPOCHS = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR YELP\n",
    "# df = pd.read_csv('train.csv', names=['label', 'review'])\n",
    "# df = df.iloc[:int(len(df)*0.05), :]\n",
    "# df['label'] -= 1\n",
    "\n",
    "# FOR TWITTER\n",
    "df = pd.read_csv('tweet_processed.csv', names=['label', 'date', 'review'])\n",
    "df = df[df.date == 6]  # if 2 days -> df.date <= 7\n",
    "df.replace([4], 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling\n",
    "v = df.label.value_counts().values\n",
    "s = df.groupby('label', group_keys=False).apply(\n",
    "    lambda x: x.sample(v[0] - v[1]))\n",
    "s = s[s.label == 0]  # choose only minor class\n",
    "\n",
    "df = df.append(s, ignore_index=True)\n",
    "\n",
    "df.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
    "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am',\n",
    "             'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
    "             'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',\n",
    "             'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than',\n",
    "             'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o',\n",
    "             're', 've', 'y', 'ma' 'st', 'nd', 'rd', 'th', \"you'll\", 'dr', 'mr', 'mrs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    \"\"\"Generate list of tokens for each sentence\n",
    "\n",
    "    Args:\n",
    "        sent (str): review\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of tokens\n",
    "    \"\"\"\n",
    "    txt = sent.lower()\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    tokens = [word for word in tokens if not word in stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df['tokens'] = df['review'].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17526641, 19057290)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V_VEC_DIM = 10\n",
    "W2V_WINDOW = 5\n",
    "W2V_MIN_COUNT = 3\n",
    "W2V_EPOCHS = 10\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    vector_size=W2V_VEC_DIM, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8)\n",
    "model.build_vocab(df['tokens'])\n",
    "model.train(df['tokens'], total_examples=model.corpus_count, epochs=W2V_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "    \"\"\"Calculate average w2v word embedding for a sentence\n",
    "\n",
    "    Args:\n",
    "        sent (list[str]): list of tokens for sentence\n",
    "\n",
    "    Returns:\n",
    "        ndarray: average word embedding\n",
    "    \"\"\"\n",
    "    wv_vec = np.zeros(W2V_VEC_DIM)\n",
    "    count = 0\n",
    "\n",
    "    for word in sent:\n",
    "        if word in model.wv:\n",
    "            count += 1\n",
    "            wv_vec += model.wv[word]\n",
    "\n",
    "    if count > 0:\n",
    "        wv_vec /= count\n",
    "    return wv_vec\n",
    "\n",
    "\n",
    "df['vector'] = df['tokens'].apply(sent_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, vector, label):\n",
    "        self.vector = vector\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vector)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vector[idx], self.label[idx]\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim_2, is_two=False,  output_dim=1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.is_two = is_two\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(\n",
    "            hidden_dim, hidden_dim_2 if is_two else output_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        if self.is_two:\n",
    "            out = self.relu2(out)\n",
    "            # out = self.dropout(out)\n",
    "            out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cuda.FloatTensor(df['vector'])\n",
    "y = torch.cuda.FloatTensor(df['label']).unsqueeze(1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = SentimentDataset(x_train, y_train)\n",
    "val_data = SentimentDataset(x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    \"\"\"Calculate accuracy \n",
    "\n",
    "    Args:\n",
    "        y_pred (Tensor): Predicted results\n",
    "        y_test (Tensor): Ground truth\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Sum of correct predictions\n",
    "    \"\"\"\n",
    "    y_h = torch.round(y_pred)\n",
    "    crct_results = (y_h == y_test).sum()\n",
    "    return crct_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLF_BATCH_SIZE = 64\n",
    "CLF_HIDDEN_LAYER1_DIM = 64\n",
    "CLF_HIDDEN_LAYER2_DIM = 16\n",
    "CLF_LEARNING_RATE = 1e-4\n",
    "CLF_EPOCHS = 500\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=CLF_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=CLF_BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(input_dim=W2V_VEC_DIM, hidden_dim=CLF_HIDDEN_LAYER1_DIM,\n",
    "                   hidden_dim_2=CLF_HIDDEN_LAYER2_DIM, is_two=True).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=CLF_LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "best_val_loss = 100\n",
    "for epoch in range(CLF_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for vecs, labels in train_loader:\n",
    "        # forward\n",
    "\n",
    "        vecs = vecs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(vecs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = binary_acc(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.float()\n",
    "        train_acc += acc.float()\n",
    "\n",
    "    train_loss /= len(train_data)\n",
    "    train_acc /= len(train_data)\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vecs, labels in val_loader:\n",
    "\n",
    "            vecs = vecs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(vecs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = binary_acc(outputs, labels)\n",
    "\n",
    "            val_loss += loss.float()\n",
    "            val_acc += acc.float()\n",
    "\n",
    "    val_loss /= len(val_data)\n",
    "    val_acc /= len(val_data)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f'Train loss: {train_loss}, Train acc: {train_acc}, Val loss: {val_loss}, Val acc: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(history['train_acc']).detach().cpu().numpy())\n",
    "plt.plot(torch.tensor(history['val_acc']).detach().cpu().numpy())\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(history['train_loss']).detach().cpu().numpy())\n",
    "plt.plot(torch.tensor(history['val_loss']).detach().cpu().numpy())\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PLStream output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('txt_tweet_daily_2days_trained.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# acc = [float(x) for x in data.split('\\n') if not x == '']\n",
    "acc = [float(x.split('-')[1])\n",
    "       for x in data.split('\\n') if not x == '']  # twitter daily\n",
    "day = [float(x.split('-')[0]) - 17 for x in data.split('\\n') if not x == '']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(day, acc, marker='.')\n",
    "plt.title('Supervised Model Accuracy over Time (2009 Apr 17 - 2009 Jun 16)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
