{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMwlak9pQwMF"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1hk_EFsBQms1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_KVuq_dRJRe"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PxdKSA6mRBRc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./train.csv', names=['label', 'review'])\n",
        "df = df.iloc[:2000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3mxkihRSJWB"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "for i in df[0]:\n",
        "  if len(i.split()) > max_len:\n",
        "    max_len = len(i.split())\n",
        "\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwNVNoEASg80"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
        "\n",
        "train_df.shape, val_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1D9TxDQS_7I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1YwYpMaTFo3"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, review, label, tokenizer, max_len):\n",
        "    self.review = review\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.review)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    review = self.review[idx]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'review': review,\n",
        "        'input_id': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'label': torch.tensor(self.label[idx], dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU3nmRXlUJZ6"
      },
      "outputs": [],
      "source": [
        "bert_model = 'bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdWiPoGeULDB"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "MAX_LEN = 128 \n",
        "\n",
        "train_data = CustomDataset(\n",
        "    review = train_df[0].values,\n",
        "                              label = train_df[1].values,\n",
        "                              tokenizer = tokenizer,\n",
        "                              max_len = MAX_LEN)\n",
        "\n",
        "val_data = CustomDataset(\n",
        "    review = val_df[0].values,\n",
        "                              label = val_df[1].values,\n",
        "                              tokenizer = tokenizer,\n",
        "                              max_len = MAX_LEN)\n",
        "\n",
        "test_data = CustomDataset(\n",
        "    review = test_df[0].values,\n",
        "                              label = test_df[1].values,\n",
        "                              tokenizer = tokenizer,\n",
        "                              max_len = MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOk2wZ_JUpXe"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e11B2LFQUsl5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC-tw-a9VjTf"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_data, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqZIsIIwWD0M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9cAYUEwWjPj"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, n_class):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = transformers.BertModel.from_pretrained(bert_model)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.linear = nn.Linear(self.bert.config.hidden_size, n_class)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    temp = self.bert(input_ids, attention_mask)\n",
        "    pooled_out = temp[1]\n",
        "    out = self.dropout(pooled_out)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjrYp4bpXde7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcxAf9_vXgwY"
      },
      "outputs": [],
      "source": [
        "n_class = 2\n",
        "model = SentimentClassifier(n_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sJiVwe9XwaG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utfksM84YF0N"
      },
      "outputs": [],
      "source": [
        "LR = 1e-5\n",
        "EPOCHS = 10\n",
        "TTL_STEPS = len(train_loader) * EPOCHS\n",
        "\n",
        "criterian = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = transformers.AdamW(params=model.parameters(), lr=LR, correct_bias=True)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=TTL_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwZ-0bSvYq3k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKySS5L_Yxm9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, criterian, optimizer, scheduler, batch_size, max_len, n_samples):\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "  correct_pred = 0\n",
        "\n",
        "  for data in data_loader:\n",
        "    input_ids = data['input_id']\n",
        "    attention_masks = data['attention_mask']\n",
        "    labels = data['label']\n",
        "\n",
        "    # forward prop\n",
        "    predictions = model(input_ids, attention_masks)\n",
        "    loss = criterian(predictions, labels)\n",
        "    _, pred_classes = torch.max(predictions, dim=1)\n",
        "\n",
        "    # back prop\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss.append(loss.item())\n",
        "    correct_pred += torch.sum(pred_classes==labels)\n",
        "\n",
        "  return correct_pred / n_samples, np.mean(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0PGnIYxZwXN"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, criterian, batch_size, max_len, n_samples):\n",
        "  model.eval()\n",
        "  eval_loss= []\n",
        "  correct_pred = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      input_ids = data['input_id']\n",
        "      attention_masks = data['attention_mask']\n",
        "      labels = data['label']\n",
        "\n",
        "      # forward prop\n",
        "      predictions = model(input_ids, attention_masks)\n",
        "      loss = criterian(predictions, labels)\n",
        "      _, pred_classes = torch.max(predictions, dim=1)\n",
        "\n",
        "      eval_loss.append(loss.item())\n",
        "      correct_pred += torch.sum(pred_classes==labels)\n",
        "\n",
        "  return correct_pred/n_samples, np.mean(eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok1HMp7nadZP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJDZNx09alHG"
      },
      "outputs": [],
      "source": [
        "history = defaultdict(list)\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'epoch: {epoch+1}/{EPOCHS}')\n",
        "\n",
        "  model = SentimentClassifier(n_class)\n",
        "  train_acc, train_loss = train_model(model, train_loader, criterian, optimizer, scheduler, BATCH_SIZE, MAX_LEN, len(train_df))\n",
        "\n",
        "  val_acc, val_loss = eval_model(model, val_loader, criterian, BATCH_SIZE, MAX_LEN, len(val_df))\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  print(f'train_acc: {train_acc}, train_lostt: {train_loss}, val_acc: {val_acc}, val_loss: {val_loss}')\n",
        "\n",
        "  if val_acc > best_acc:\n",
        "    best_model_name = f'best_model_{val_acc}.bin'\n",
        "    torch.save(model.state_dict(), best_model_name)\n",
        "    best_acc = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4TF7q3ob5Wq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
